<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>Integrably Sorry - Dailies</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">Integrably Sorry</a></h1>
                <nav><ul>
                    <li><a href="/pages/about.html">About</a></li>
                    <li class="active"><a href="/category/dailies.html">Dailies</a></li>
                    <li><a href="/category/life.html">Life</a></li>
                    <li><a href="/category/math.html">Math</a></li>
                    <li><a href="/category/programming.html">Programming</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/miniai-learner.html">MiniAI learner</a></h1>
<footer class="post-info">
        <abbr class="published" title="2024-08-04T13:50:00+03:00">
                Published: Sun 04 August 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ofer-yehuda.html">Ofer Yehuda</a>
        </address>
<p>In <a href="/category/dailies.html">Dailies</a>.</p>

</footer><!-- /.post-info --><p>Today I'm recreating the <em>learner</em> framework from the FastAI course. It's a flexible and quite powerful abstraction around the optimization of the DNN model, which streamlines the user experience. For example, it will be very easy to add different logging capabilities, learning rate finder etc. It is built during the lesson, but there are a lot of moving parts, and a lot of usage of advanced python, which is both good and bad: good, because the code is quite elegant. Bad, because it's harder to reason about and debug, at least, that's how I feel right now. Perhaps it will change as I build it.</p>
<p>What is the learner abstraction comprised of?
- we break the training process <code>fit</code>, <code>fit_epoch</code> and <code>fit_batch</code>.
- we add a callback system by emitting signals before and after each stage and calling relevant callbacks
- we use exceptions as a control mechanism for the callbacks</p>
<hr>

<p>It's hard to say what goes where, ie how are arguments shared. Looks like the reference implementation goes all in on state, so nothing is passed as function arguments. There's even iteration with object members, eg</p>
<pre><code class="language-python">for self.epoch in range(n_epochs):
    ...
</code></pre>
<hr>

<p>I'm creating a synthetic dataset to have something to play around with. </p>
<pre><code class="language-python">from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split

X = np.linspace(-1, 1, num=1000)
f = lambda x: np.sin(x)
y = f(X)
ds = TensorDataset(tensor(X), tensor(y))
train, val = random_split(ds, [0.9, 0.1])
</code></pre>
<hr>

<p>What we have so far</p>
<pre><code class="language-python">class Learner:
  def __init__(self, model, dls, loss, lr, opt_func):
    fc.store_attr()
    self.opt = opt_func(model.parameters(), lr=lr)

  def one_batch(self):
    x, y = self.batch
    self.opt.zero_grad()
    y_pred = model(x)
    l = self.loss(y_pred, y)
    l.backward()
    self.opt.step()

  def one_epoch(self):
    for self.i, self.batch in enumerate(self.dls.train):
      self.one_batch()

  def fit(self, n_epochs):
    for self.epoch in range(n_epochs):
      self.one_epoch()
</code></pre>
<p>I'm trying not to just copy-paste the code from the original notebooks, so I'm running into a lot of "design" questions. For example, how should we track the losses? we need to differentiate validation and training, we need to normalize by batch size...</p>
<hr>

<p>Updated version:</p>
<pre><code class="language-python">class Learner:
  def __init__(self, model, dls, loss_f, lr, opt_func=optim.SGD): fc.store_attr()

  def one_batch(self):
    self.xb, self.yb = to_device(self.batch)
    self.preds = model(self.xb)
    self.loss = self.loss_f(self.preds, self.yb)
    if self.model.training:
      self.loss.backward()
      self.opt.step()
      self.opt.zero_grad()
    with torch.no_grad(): self.calc_stats()

  def calc_stats(self):
    acc = (self.preds.argmax(dim=1)==self.yb).float().sum()
    self.accs.append(acc)
    n = len(self.xb)
    self.losses.append(self.loss*n)
    self.ns.append(n)

  def one_epoch(self, train):
    self.model.training = train
    dl = self.dls.train if train else self.dls.valid
    for self.i, self.batch in enumerate(self.dls.train):
      self.one_batch()
    n = sum(self.ns)
    print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)

  def fit(self, n_epochs):
    self.losses = []
    self.model.to(def_device)
    self.ns = []
    self.accs = []
    self.opt = self.opt_func(model.parameters(), lr=lr)
    for self.epoch in range(n_epochs):
      self.one_epoch(True)
      with torch.no_grad(): self.one_epoch(False)

</code></pre>
<p>This is now pretty much aligned with the first version in the notebook. What did we change?
- everything is basically a state variable: the batch, even the input and label of the batch, the loss, the preds. 
- train/validation logic: <code>one_batch</code> performs backprop only when model is set to training.
- handles device moving
- calc_stats function does all that stat tracking. keeps track of sums, when printing it calculates the weighted mean (weighted by the batch size)
    - I notice now that every epoch includes previous epochs' losses and accuracies. This is not intended behavior, right?</p>
<p>There are some problems with our learner: first of all, the task I test on is regression, so it makes no sense to calculate accuracy, and what if we are doing autoencoding or another task without labels. Also, everything is hardcoded, so it's not easy to change stuff without creating copies.</p>
<p>Our next step will be to start moving things into callbacks. This will allow us to modularize some of the code.</p>
<hr>

<pre><code class="language-python">from operator import attrgetter

class Learner:
  def __init__(self, model, dls, loss_f, lr, opt_func=optim.SGD, cbs=[]): fc.store_attr()

  def one_batch(self):
    self.callback('before_batch')
    self.xb, self.yb = to_device(self.batch)
    self.preds = model(self.xb)
    self.loss = self.loss_f(self.preds, self.yb)
    if self.model.training:
      self.loss.backward()
      self.opt.step()
      self.opt.zero_grad()
    self.callback('after_batch')

  def one_epoch(self, train):
    self.callback('before_epoch')
    self.model.training = train
    dl = self.dls.train if train else self.dls.valid
    for self.i, self.batch in enumerate(self.dls.train):
      self.one_batch()
    n = sum(self.ns)
    self.callback('after_epoch')

  def callback(self, stage):
    for c in sorted(self.cbs, key=attrgetter('order')):
      if hasattr(c, stage):
        getattr(c, stage)(self)

  def fit(self, n_epochs):
    self.losses = []
    self.model.to(def_device)
    self.ns = []
    self.accs = []
    self.opt = self.opt_func(model.parameters(), lr=lr)
    self.callback('before_fit')
    for self.epoch in range(n_epochs):
      self.one_epoch(True)
      with torch.no_grad(): self.one_epoch(False)
    self.callback('after_fit')
</code></pre>
<p>Now we added signals for before and after each stage, and a function <code>callback</code> that "emits" the signal, ie call every callback that listens to that signal (defined as having a method with the same name). The metrics have been moved to a callback that looks like </p>
<pre><code class="language-python">class Metrics:
  order = 0

  def after_batch(self, l):
    with torch.no_grad():
      n = len(l.xb)
      acc = (l.preds.argmax(dim=1) == l.yb).float().sum()
      self.accs.append(acc)
      self.losses.append(l.loss * n)
      self.ns.append(n)

  def before_epoch(self, l):
    self.accs = []
    self.losses = []
    self.ns = []

  def after_epoch(self, l):
    n = sum(self.ns)
    print(l.epoch, l.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)
</code></pre>
<p>The <code>order</code> variable determines the order in which callbacks are called.</p>
<p>What's our next step? we could try out some callbacks, or make the code "nicer" with context managers, add control through exceptions or abstract some of the model calls like predict so we can support more general models.</p>
<p>Let's write a callback for activation monitoring. We'll use pytorch hooks. A hook is a function with the following signature</p>
<pre><code class="language-python">hook(module, input, output)

# registering a hook
handle = module.register_forward_hook(hook)

# remove hook
handle.remove()
</code></pre>
<p>We can create an abstraction that handles that for us</p>
<pre><code class="language-python">class Hook():
    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))
    def remove(self): self.hook.remove()
    def __del__(self): self.remove()
</code></pre>
<p>And then an <code>Activations</code> callback</p>
<pre><code class="language-python">class Activations():
  order = 0

  def calc_stats(self, i, hook, module, inp, outp):
    self.means[i].append(outp.mean().item())
    self.stds[i].append(outp.std().item())

  def before_fit(self, l):
    self.means = [[] for _ in l.model]
    self.stds = [[] for _ in l.model]
    self.hooks = [Hook(m, partial(self.calc_stats, i)) for i, m in enumerate(l.model)]

  def after_fit(self, l):
    del self.hooks
</code></pre>
<div style="display: flex; justify-content: center; width: 100%; margin: auto;">
  <img src="/images/act_means.png" style="width: 50%; margin: 5px;" />
  <img src="/images/act_std.png" style="width: 50%; margin: 5px;" />
</div>

<!--![[act_std.png]]![[act_means.png]]-->                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/a-naive-autoencoder-on-fashionmnist.html" rel="bookmark"
                           title="Permalink to A naive autoencoder on FashionMNIST">A naive autoencoder on FashionMNIST</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2024-07-31T14:53:00+03:00">
                Published: Wed 31 July 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ofer-yehuda.html">Ofer Yehuda</a>
        </address>
<p>In <a href="/category/dailies.html">Dailies</a>.</p>

</footer><!-- /.post-info -->                <p>Today we'll recreate the fastai <a href="https://github.com/fastai/course22p2/blob/master/nbs/08_autoencoder.ipynb">notebook on autoencoders</a>, where we train a vanilla autoencoder in FashionMNIST. Even though the autoencoder was actually doing a pretty bad job, it will be good practice for working with HuggingFace databases, CNNs and autoencoders.</p>
<h2>Getting the data</h2>
<pre><code class="language-python">import datasets
from torch.utils.data import …</code></pre>
                <a class="readmore" href="/a-naive-autoencoder-on-fashionmnist.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/building-up-pytorch-abstractions-part-1.html" rel="bookmark"
                           title="Permalink to Building up PyTorch abstractions: Part 1">Building up PyTorch abstractions: Part 1</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2024-07-28T18:01:00+03:00">
                Published: Sun 28 July 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ofer-yehuda.html">Ofer Yehuda</a>
        </address>
<p>In <a href="/category/dailies.html">Dailies</a>.</p>

</footer><!-- /.post-info -->                <p>Today we will retrace lesson <a href="https://www.youtube.com/watch?v=vGdB4eI4KBs">13</a>-<a href="https://www.youtube.com/watch?v=veqj0DsZSXU">14</a>'s notebook that "builds up" pytorch abstractions from scratch. As a first step we'll rederive everything in hardcore numpy (maybe hardcore should be reserved for C). Then we'll start building the abstractions.</p>
<p>First up we load <code>mnist</code> data:</p>
<pre><code class="language-python">from pathlib import Path
from …</code></pre>
                <a class="readmore" href="/building-up-pytorch-abstractions-part-1.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/debugging-session-logseq-omnivore-plugin.html" rel="bookmark"
                           title="Permalink to Debugging session: Logseq Omnivore plugin">Debugging session: Logseq Omnivore plugin</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2024-07-22T14:24:00+03:00">
                Published: Mon 22 July 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ofer-yehuda.html">Ofer Yehuda</a>
        </address>
<p>In <a href="/category/dailies.html">Dailies</a>.</p>

</footer><!-- /.post-info -->                <p>I'm trying to debug a weird issue with the Logseq omnivore plugin where it takes forever to sync and it seemingly creates and deletes pages needlessly.</p>
<p>My first step was to properly setting up a dev env (<code>pnpm dev</code>) which didn't work out of the box, instead of just building …</p>
                <a class="readmore" href="/debugging-session-logseq-omnivore-plugin.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/rnn-generations.html" rel="bookmark"
                           title="Permalink to RNN generations">RNN generations</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2024-07-20T17:51:00+03:00">
                Published: Sat 20 July 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ofer-yehuda.html">Ofer Yehuda</a>
        </address>
<p>In <a href="/category/dailies.html">Dailies</a>.</p>

</footer><!-- /.post-info -->                <p>On advice from my uncle I'm continuing to fallback on task difficulty with RNNs.</p>
<p>Unc's tips:
- Swirch to generation task
- Try residuals
- Go deeper
- Add projections 
- No dropout?</p>
<p>Let's recreate Karpathy's classic post and train a language model on tiny-shakespeare. We can get the entire dataset which is a text …</p>
                <a class="readmore" href="/rnn-generations.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/recreating-stable-diffusions-pipeline.html" rel="bookmark"
                           title="Permalink to Recreating Stable Diffusion's Pipeline">Recreating Stable Diffusion's Pipeline</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2024-07-18T15:56:00+03:00">
                Published: Thu 18 July 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ofer-yehuda.html">Ofer Yehuda</a>
        </address>
<p>In <a href="/category/dailies.html">Dailies</a>.</p>

</footer><!-- /.post-info -->                <p>Today I'm going to recreate the pipeline shown in <a href="https://course.fast.ai/Lessons/lesson10.html">lesson 10 of the fast.ai course</a>. We'll go through what's needed on the high-level, using pretrained models for everything. The pipeline is fed in a text prompt and it produces an image. A prompt means we need a tokenizer to …</p>
                <a class="readmore" href="/recreating-stable-diffusions-pipeline.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/shifting-to-translation-with-rnns.html" rel="bookmark"
                           title="Permalink to Shifting to translation with RNNs">Shifting to translation with RNNs</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2024-07-15T16:53:00+03:00">
                Published: Mon 15 July 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ofer-yehuda.html">Ofer Yehuda</a>
        </address>
<p>In <a href="/category/dailies.html">Dailies</a>.</p>

</footer><!-- /.post-info -->                <p>I'm pivoting the RNN summarization code to an easier example - Machine translation. Easier in the sense of the <a href="https://huggingface.co/datasets/yhavinga/ccmatrix">dataset</a>, which consists of much shorter en-de sentence pairs compared to the summarization task. I have some suspicion that the there is a bug or something in my code, so today, after …</p>
                <a class="readmore" href="/shifting-to-translation-with-rnns.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/cnn-summarization-task.html" rel="bookmark"
                           title="Permalink to CNN summarization task">CNN summarization task</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2024-07-12T21:43:00+03:00">
                Published: Fri 12 July 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ofer-yehuda.html">Ofer Yehuda</a>
        </address>
<p>In <a href="/category/dailies.html">Dailies</a>.</p>

</footer><!-- /.post-info -->                <p>Today we're gonna dip our fingers into the first generative NLP task - text summarization. We're gonna use the <a href="https://github.com/abisee/cnn-dailymail">CNN/Daily Mail dataset</a> as done in <a href="https://arxiv.org/pdf/1704.04368">this paper</a>. Let's get to it.</p>
<h2>Data prep</h2>
<p>I started by doing all the preprocessing of the files myself, but then found a the dataset …</p>
                <a class="readmore" href="/cnn-summarization-task.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/a-tidbit-about-docker-containers.html" rel="bookmark"
                           title="Permalink to A tidbit about docker containers">A tidbit about docker containers</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2024-04-02T18:30:00+03:00">
                Published: Tue 02 April 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ofer-yehuda.html">Ofer Yehuda</a>
        </address>
<p>In <a href="/category/dailies.html">Dailies</a>.</p>

</footer><!-- /.post-info -->                <p>I played around with docker containers today. Tried to install mssql on an M2 mac. Didn't go smoothly at first, but I attribute it to inexperience. The arm architecture crops up here and there when you're on one of those. Anyway, cool thing:</p>
<p>You can run a terminal inside a …</p>
                <a class="readmore" href="/a-tidbit-about-docker-containers.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/the-mysteries-of-html-box-heights.html" rel="bookmark"
                           title="Permalink to The mysteries of HTML box heights">The mysteries of HTML box heights</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2024-04-02T18:30:00+03:00">
                Published: Tue 02 April 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ofer-yehuda.html">Ofer Yehuda</a>
        </address>
<p>In <a href="/category/dailies.html">Dailies</a>.</p>

</footer><!-- /.post-info -->                <p>Today's post is not "fully cooked", I still don't think I figured this topic out, but I did learn a few things.</p>
<p>Today I needed to do some css hacking for a dashboard I'm building with <code>streamlit</code>. I was using a <code>grid</code> element from the streamlit-extras package, and I wanted …</p>
                <a class="readmore" href="/the-mysteries-of-html-box-heights.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
<p class="paginator">
    Page 1 / 2
        <a href="/category/dailies2.html">&raquo;</a>
        <a href="/category/dailies2.html">&#8649;</a>
</p>
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://github.com/ofer1992">github</a></li>
                            <li><a href="https://x.com/oferyehuda">twitter</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>