<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>Recreating Stable Diffusion's Pipeline</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <meta name="description" content="Today I'm going to recreate the pipeline shown in lesson 10 of the fast.ai course. We'll go through what's needed on the high-level, using..." />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">Integrably Sorry</a></h1>
                <nav><ul>
                    <li><a href="/pages/about.html">About</a></li>
                    <li class="active"><a href="/category/dailies.html">Dailies</a></li>
                    <li><a href="/category/life.html">Life</a></li>
                    <li><a href="/category/math.html">Math</a></li>
                    <li><a href="/category/programming.html">Programming</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/recreating-stable-diffusions-pipeline.html" rel="bookmark"
           title="Permalink to Recreating Stable Diffusion's Pipeline">Recreating Stable Diffusion's Pipeline</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2024-07-18T15:56:00+03:00">
                Published: Thu 18 July 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ofer-yehuda.html">Ofer Yehuda</a>
        </address>
<p>In <a href="/category/dailies.html">Dailies</a>.</p>

</footer><!-- /.post-info -->      <p>Today I'm going to recreate the pipeline shown in <a href="https://course.fast.ai/Lessons/lesson10.html">lesson 10 of the fast.ai course</a>. We'll go through what's needed on the high-level, using pretrained models for everything. The pipeline is fed in a text prompt and it produces an image. A prompt means we need a tokenizer to convert the text into indexes from a vocabulary, and we need an encoder to create a text embedding.</p>
<h2>Text embedding</h2>
<pre><code class="language-python">from transfomers import CLIPTextModel, CLIPTokenizer

tokenizer = CLIPTokenizer.from_pretrained(&quot;openai/clip-vit-large-patch14&quot;, torch_dtype=torch.float16)
text_encoder = CLIPTextModel.from_pretrained(&quot;openai/clip-vit-large-patch14&quot;, torch_dtype=torch.float16).cuda()
</code></pre>
<p>Pretty straightforward, loading models from hugging face. We use half-precision to save GPU memory.</p>
<p>Now the processing</p>
<pre><code class="language-python">prompt = [&quot;An image of a big truck&quot;]*4
tokenized = tokenizer(prompt, padding=&quot;max_length&quot;, max_length=tokenizer.model_max_length, truncation=True, return_tensors=&quot;pt&quot;)
tokenized['input_ids']
</code></pre>
<p>Which returns</p>
<pre><code>tensor([[49406, 550, ..., 49407], ...])
</code></pre>
<p>Because we are padding the tensor, an <code>&lt;end-sentence&gt;</code> is used as the padding value.</p>
<pre><code class="language-python">t_enc = text_encoder(tokenized['input_ids'].cuda())
t_enc.last_hidden_state
</code></pre>
<p>The encoder turns each token to a size 768 vector.
Q: the model returns a last_hidden_state and a pooler_output. What's the difference?</p>
<h3>The Autoencoder</h3>
<p>As mentioned in the lecture, the diffusion process takes place not in the image space but in a latent space which is 64 times smaller, so we can run it at home. To get the latents, we use a variational auto-encoder. Let's set it up</p>
<pre><code class="language-python">from diffusers import AutoencoderKL

vae = AutoencoderKL.from_pretrained(&quot;stabilityai/sd-vae-ft-ema&quot;, torch_dtype=torch.float16).cuda()
</code></pre>
<p>Again we use half-precision, and we use a fine-tuned vae from stability ai. In general we need to use the same components of the pipeline since they were trained together. Let's look at the flow</p>
<pre><code class="language-python">with torch.no_grad():
    enc_im = vae.encode(im_t.unsqueeze(0).cuda())
    dec_im = vae.decode(enc_im.latent_dist.mean)
</code></pre>
<p>We feed the encoder an image tensor and get back a latent. It's actually a distribution or something, I don't really know, but what we care about is the mean. We pass it on to the decoder and we get back</p>
<p style="width:50%; margin:auto">
  <img src="/images/compare_lizards.png" />
  <figcaption>By order of appearance: original, decoded, diff</figcaption>
</p>

<!--![[compare_lizards.png]]-->

<p>Seems like the compression losses some of the high-frequency information, but otherwise it's a pretty good job.</p>
<h2>The diffusion process and the scheduler</h2>
<p>The diffusion process starts from gaussian noise. In our case, the noise exists in the latent space, so we initialize the latents which are in the shape of <code>(4, 32, 32)</code>. </p>
<pre><code class="language-python">gen = torch.random.manual_seed(42)
latents = torch.randn(4, 4, 64, 64, generator=gen)
</code></pre>
<p>What is this scheduler? sadly this is a bit confusing, so let's try to break it down. Theoretically, what the diffusion model learns is to "reverse" a diffusion process defined as
</p>
<div class="math">$$
x^{(t)}=\sqrt{1-\beta_t}x^{(t-1)}+\beta_{t}\varepsilon
$$</div>
<p>
where <span class="math">\(x^{(0)}\)</span> is a clean normal image, <span class="math">\(\varepsilon\)</span> is standard gaussian noise, and <span class="math">\(\beta_t\)</span> is a coefficient that determines the strength of the "noising" at time step <span class="math">\(t\)</span>. Mathematically, if the betas are small enough and the amount of steps taken <span class="math">\(T\)</span> is large enough then <span class="math">\(x^{(T)}\sim\mathcal{N}(0, 1)\)</span>. This process is also known as the "forward process". If we knew the noise added, we could "reverse" the process by calculating
</p>
<div class="math">$$
x^{(t-1)}=\left(x^{(t)}-\beta_t\varepsilon\right)/\sqrt{1-\beta_t}
$$</div>
<p>
And that's the task of the network: given <span class="math">\(x^{(t)}/\sqrt{1-\beta_t}\)</span>, it predicts the noise sample added <span class="math">\(\varepsilon\)</span>.</p>
<p><strong>Comment</strong>: looking into the scheduler a bit more, I see this goes much deeper than I thought (ode derivatives and stuff), so this explanation is probably wrong. I hope to catch up on this eventually, but for now let's get back to the practicalities.</p>
<p>First we initialize the U-Net which is the model that actually predicts the noise.</p>
<pre><code class="language-python">from diffusers import UNet2DConditionModel
unet = UNet2DConditionModel.from_pretrained(&quot;CompVis/stable-diffusion-v1-4&quot;, subfolder=&quot;unet&quot;, torch_dtype=torch.float16).to(&quot;cuda&quot;)
</code></pre>
<p>Before we move on to the inference loop, there's one last detail: in order to steer the model to generate the image according to our prompt, it's not enough to feed in the prompt. We actually feed in two prompts, our own and an empty prompt. We then combine the noise prediction for the empty prompt and the desired prompt in a way that guides the model. We'll see how it's done now</p>
<pre><code class="language-python">height = 512
width = 512
num_inference_steps = 70
guidance_scale = 7.5

scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=&quot;scaled_linear&quot;, num_train_timesteps=1000)
scheduler.set_timesteps(num_inference_steps)

# initialize the latents as standard normal distribution
gen = torch.random.manual_seed(44)
latents = torch.randn(batch_size, 4, height // 8, width // 8, generator=gen).half().cuda()
latents = latents * scheduler.init_noise_sigma # scale the latents to the initial noise value

for i, t in tqdm(enumerate(scheduler.timesteps)):
    input = torch.cat([latents] * 2)
    input = scheduler.scale_model_input(input, t)

    # predict the noise residual
    with torch.no_grad(): 
        pred = unet(input, t, encoder_hidden_states=text_embeddings).sample

    # perform guidance
    pred_uncond, pred_text = pred.chunk(2)
    pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)

    # compute the &quot;previous&quot; noisy sample
    latents = scheduler.step(pred, t, latents).prev_sample
</code></pre>
<p>Running this for a prompt like "A wild pokemon appears!" generates</p>
<p style="width:50%; margin:auto">
  <img src="/images/wild_pokemon.png" />
</p>
<!--![[wild_pokemon.png]]-->

<p>Wrapping up, if you ended up reading this, I recommend to look at the lecture or at least the notebooks, where you can find an actual working implementation and not just snippets.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://github.com/ofer1992">github</a></li>
                            <li><a href="https://x.com/oferyehuda">twitter</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>